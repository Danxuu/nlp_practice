{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXygp44-cK-N",
    "outputId": "3b2be4f6-d833-4e45-e755-fdb82c9f553d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xudan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xudan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/xudan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "#svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lIc9If0LiaW8"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "kGyxozdtffHD",
    "outputId": "3ab7ac76-c354-4a9c-de7b-9bdc8eeb4cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('/content')\n",
    "df_train=pd.read_csv('train.csv')\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "YoFWWJd3ghCm",
    "outputId": "b82941e3-a3a4-4f74-eb7d-9d43ffec3d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xudan/miniconda3/envs/py38/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='target'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CLASS DISTRIBUTION\n",
    "#if dataset is balanced or not\n",
    "x=df_train['target'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D92jrzvOjX0n",
    "outputId": "8565e526-9828-4832-bd5f-e54c172f0fce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Missing values\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBYekfLljco3",
    "outputId": "5d1c0b5c-a989-4e70-d523-790492937678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.167532864567411\n",
      "14.704744357438969\n",
      "108.11342097217977\n",
      "95.70681713496084\n",
      "14.664934270865178\n",
      "14.09649930907416\n"
     ]
    }
   ],
   "source": [
    "#1. WORD-COUNT\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "print(df_train[df_train['target']==1]['word_count'].mean()) #Disaster tweets\n",
    "print(df_train[df_train['target']==0]['word_count'].mean()) #Non-Disaster tweets\n",
    "#Disaster tweets are more wordy than the non-disaster tweets\n",
    "\n",
    "#2. CHARACTER-COUNT\n",
    "df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n",
    "print(df_train[df_train['target']==1]['char_count'].mean()) #Disaster tweets\n",
    "print(df_train[df_train['target']==0]['char_count'].mean()) #Non-Disaster tweets\n",
    "#Disaster tweets are longer than the non-disaster tweets\n",
    "\n",
    "#3. UNIQUE WORD-COUNT\n",
    "df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "print(df_train[df_train['target']==1]['unique_word_count'].mean()) #Disaster tweets\n",
    "print(df_train[df_train['target']==0]['unique_word_count'].mean()) #Non-Disaster tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "SrwNWje1j_VJ",
    "outputId": "bd67e4c6-9379-4a17-afc0-f82e6bdb1d17"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f9/5h3swd8x39n00m3ry2hgf1g80000gn/T/ipykernel_31142/3053463960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plotting word-count per tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Disaster tweets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Plotting word-count per tweet\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=df_train[df_train['target']==1]['word_count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('Disaster tweets')\n",
    "train_words=df_train[df_train['target']==0]['word_count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Non-disaster tweets')\n",
    "fig.suptitle('Words per tweet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJ_nyXpSmgTo",
    "outputId": "e1d78ba1-0133-4357-a878-8b7ccf127af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "#1. Common text preprocessing\n",
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n",
    "\n",
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace \n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text\n",
    "\n",
    "text=preprocess(text)\n",
    "print(text)  #text is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwPWurZ6w9OV",
    "outputId": "3fd9894c-3d5a-4684-cb91-44077da57ac9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/xudan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PboCAb_0v8EG",
    "outputId": "375a56a3-6a1f-4e52-b012-4090f1208e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message cleaned may involve things like adjacent spaces tabs\n",
      "messag clean may involv thing like adjac space tab\n",
      "messag clean may involv thing like adjac space tab\n"
     ]
    }
   ],
   "source": [
    "#3. LEXICON-BASED TEXT PROCESSING EXAMPLES\n",
    " \n",
    "#1. STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "text=stopword(text)\n",
    "print(text)\n",
    "\n",
    "#2. STEMMING\n",
    " \n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "text=stemming(text)\n",
    "print(text)\n",
    "\n",
    "#3. LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "text = lemmatizer(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6s_2KALCzDsO",
    "outputId": "8fc2d9d0-6bc6-4502-f157-4eb86e432218"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \n",
       "0       1         deed reason earthquake may allah forgive u  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  resident ask shelter place notify officer evac...  \n",
       "3       1  people receive wildfire evacuation order calif...  \n",
       "4       1  get sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FINAL PREPROCESSING\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "\n",
    "df_train['clean_text'] = df_train['text'].apply(lambda x: finalpreprocess(x))\n",
    "df_train=df_train.drop(columns=['word_count','char_count','unique_word_count'])\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NnUcW4fza0N",
    "outputId": "f6d71ef9-3cf8-4117-d54e-8b32b88fda3c"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f9/5h3swd8x39n00m3ry2hgf1g80000gn/T/ipykernel_31142/186368954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#combination of word and its vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#for converting sentence to vectors/numbers from word vectors result by Word2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mindex2word\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mmatrix_nonzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatrix_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0mnum_skipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0;31m# Decide the order of rows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "# create Word2vec model\n",
    "#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
    "#length of words_f is number of documents/sentences in your dataset\n",
    "df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
    "model = Word2Vec(df_train['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))  #combination of word and its vector\n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtI4UVaYznfb"
   },
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Input: \"reviewText\", \"rating\" and \"time\"\n",
    "# Target: \"log_votes\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train[\"clean_text\"],\n",
    "                                                  df_train[\"target\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  shuffle=True)\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
    "\n",
    "#TF-IDF\n",
    "# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n",
    "# Only transform x_test (not fit and transform)\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will \n",
    "#change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n",
    "#fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without \n",
    "#it, and the have compatible\n",
    "\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_val_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "So25cowHzsxg",
    "outputId": "7bab6cef-ef29-4ec6-ffe2-36a9939a4bac"
   },
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3cp2D8z06tu",
    "outputId": "9a6ad884-ddd7-4736-c86e-4fb2cb5204a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83       882\n",
      "           1       0.82      0.64      0.72       641\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.77      0.77      1523\n",
      "weighted avg       0.79      0.79      0.78      1523\n",
      "\n",
      "Confusion Matrix: [[790  92]\n",
      " [232 409]]\n",
      "AUC: 0.83996713610041\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPqBM8jL1A-0",
    "outputId": "696cd894-4939-46a5-9be6-688210e2365f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.84      0.73       882\n",
      "           1       0.63      0.38      0.47       641\n",
      "\n",
      "    accuracy                           0.65      1523\n",
      "   macro avg       0.64      0.61      0.60      1523\n",
      "weighted avg       0.64      0.65      0.62      1523\n",
      "\n",
      "Confusion Matrix: [[739 143]\n",
      " [397 244]]\n",
      "AUC: 0.6786412953116764\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjz3VFQj1FoN",
    "outputId": "07e6d425-c448-41d1-95bf-2eee31086f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword  ... predict_prob target\n",
      "0   0     NaN  ...     0.863654      1\n",
      "1   2     NaN  ...     0.754733      1\n",
      "2   3     NaN  ...     0.937769      1\n",
      "3   9     NaN  ...     0.780818      1\n",
      "4  11     NaN  ...     0.996700      1\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Testing it on new dataset with the best model\n",
    "df_test=pd.read_csv('test.csv')  #reading the data\n",
    "df_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=df_test['clean_text'] \n",
    "X_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\n",
    "y_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['target']= y_predict\n",
    "print(df_test.head())\n",
    "final=df_test[['id','target']].reset_index(drop=True)\n",
    "final.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "N_icWqwh2LxI"
   },
   "outputs": [],
   "source": [
    "final=df_test[['id','keyword','location','text','clean_text','predict_prob','target']].reset_index(drop=True)\n",
    "final.to_csv('submissionfull.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
